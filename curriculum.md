# 4-Week Curriculum: Building a Sea Turtle Re‑Identification System with Neural Networks

This 4-week curriculum will guide a medior-level developer through the process of designing, building, and evaluating a deep learning system that can recognize individual sea turtles from images. We will progress from basic CNN classifiers to advanced face-recognition-style identification techniques, emphasizing hands-on prototyping and gradually evolving the system’s capabilities. Each week has clear goals, key topics, practical exercises, and curated resources. By the end, you’ll have a proof-of-concept pipeline for sea turtle re-identification that can handle a growing catalog of individuals.

## Week 1: Data Preparation and Baseline CNN Models

**Goals:** Establish a strong foundation in the problem domain and data handling. This week you will curate and preprocess your sea turtle image dataset, apply augmentations to tackle underwater imaging challenges, and implement a simple baseline CNN to classify individual turtles. The goal is to become familiar with the data and obtain an initial benchmark model.

**Key Topics:**

- **Understanding the Domain & Data:** Learn what makes individual turtle identification possible. For example, the scale patterns on a turtle’s face are unique and remain stable over decades ([Advancing conservation with AI-based facial recognition of turtles - Google DeepMind](https://deepmind.google/discover/blog/advancing-conservation-with-ai-based-facial-recognition-of-turtles/#:~:text=Given%20the%20additional%20challenge%20of,decade%20lifespan)). Recognize common challenges in underwater photos: varying lighting (e.g. color shifts under water), different angles/viewpoints of the turtle, motion blur, and occlusions (parts of the turtle obscured by marine growth or other objects).
- **Data Organization:** Set up your dataset with a clear structure (e.g. a folder per turtle ID). Ensure you have balanced representation per individual if possible. Consider how new images/IDs will be added over time.
- **Preprocessing & Augmentation:** Dive into image preprocessing techniques. This includes normalization and resizing images to a consistent size for your network. Apply data augmentations to make your model robust to real-world variability. For sea turtle images, useful augmentations include random rotations and flips (to handle different orientations), color jittering (adjust brightness, contrast, hue to simulate underwater lighting changes), adding noise or blur (to mimic water distortion), and random crops or occlusions (to train against partial views). Heavy augmentation can greatly improve generalization ([GitHub - ahmedbesbes/whales-classification: My solution to the Global Data Science Challenge](https://github.com/ahmedbesbes/whales-classification#:~:text=,the%20model%20stability%20and%20generalization)). For example, one wildlife ID project used augmentations like Gaussian noise, blurring, color shifts, brightness/hue changes, and even simulated rain/splash effects ([GitHub - ahmedbesbes/whales-classification: My solution to the Global Data Science Challenge](https://github.com/ahmedbesbes/whales-classification#:~:text=augmentation,saturation%2C%20sharpening%2C%20perspective%20and%20elastic)) ([GitHub - ahmedbesbes/whales-classification: My solution to the Global Data Science Challenge](https://github.com/ahmedbesbes/whales-classification#:~:text=,the%20model%20stability%20and%20generalization)).
- **Baseline CNN Architecture:** Review the basics of convolutional neural networks and why they work well for image recognition. Implement a simple CNN (with a few conv and pooling layers) or use a small pre-trained model, to classify images by individual. At this stage, treat it as a closed-set classification problem (each known turtle is a class). This will be a baseline to improve upon.
- **Baseline Evaluation:** Since many individuals exist, a simple accuracy on a validation set (e.g. percentage of correctly classified images) can serve as a metric. Keep this baseline result for comparison in later weeks.

**Practical Exercises:**

1. **Dataset Setup:** Load your turtle image dataset. Write a small script to inspect the data (image dimensions, number of images per turtle, etc.) and plot a few examples. Verify the labeling (class per individual).
2. **Preprocessing Pipeline:** Use a library like **Albumentations** or **TensorFlow ImageDataGenerator** to create an augmentation pipeline. For a given sample image, apply a series of augmentations (rotation, color jitter, etc.) and visualize the results to ensure they make sense (e.g., simulate the kinds of variability you expect in underwater photos).
3. **Build and Train a CNN:** Using your framework of choice (PyTorch or TensorFlow), code a small CNN model. For instance, in PyTorch define a network with 2–3 convolutional layers (ReLU activations and max-pooling) followed by a couple of fully connected layers. Train this model on your dataset (or a subset if the full set is large) for a few epochs to see it can distinguish some individuals. Monitor training vs validation accuracy to check for overfitting.
4. **Baseline Results:** Evaluate the model on a validation split or a hold-out set of turtle images. Record the accuracy (or other relevant metrics) as baseline. Also, save this model; it will be a reference point for improvements.

**Resources:**

- **Albumentations Documentation** – Learn about various image augmentations (e.g. `RandomBrightnessContrast`, `HueSaturationValue`, flips, blurs) and how to apply them ([GitHub - ahmedbesbes/whales-classification: My solution to the Global Data Science Challenge](https://github.com/ahmedbesbes/whales-classification#:~:text=augmentation,saturation%2C%20sharpening%2C%20perspective%20and%20elastic)). The official docs and examples are a great starting point ([What is image augmentation - Albumentations Documentation](https://albumentations.ai/docs/introduction/image_augmentation/#:~:text=What%20is%20image%20augmentation%20,slightly%20change%20the%20original%20image)) ([Improving Semantic Segmentation Performance in Underwater Images](https://www.mdpi.com/2077-1312/11/12/2268#:~:text=Improving%20Semantic%20Segmentation%20Performance%20in,be%20achieved%20with%20both%20strategies)).
- **Tutorial: Training a CNN Classifier** – If using PyTorch, follow the [Transfer Learning for Computer Vision Tutorial (PyTorch official)](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) for an example of loading data and training a model (it covers training a ResNet on a small dataset, which can be adapted to a simple CNN or used in Week 2) ([Transfer Learning for Computer Vision Tutorial - PyTorch](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#:~:text=In%20this%20tutorial%2C%20you%20will,image%20classification%20using%20transfer%20learning)). For TensorFlow, see Keras’ [Basic Image Classification](https://www.tensorflow.org/tutorials/images/classification) guide.
- **Sea Turtle ID Background** – The DeepMind blog *“Advancing conservation with AI-based facial recognition of turtles”* gives a non-technical overview of why turtle face patterns enable re-identification ([Advancing conservation with AI-based facial recognition of turtles - Google DeepMind](https://deepmind.google/discover/blog/advancing-conservation-with-ai-based-facial-recognition-of-turtles/#:~:text=Given%20the%20additional%20challenge%20of,decade%20lifespan)) (good for understanding the problem significance).
- **PyTorch Lightning (optional this week)** – If you prefer a high-level training framework, skim the [PyTorch Lightning Tutorial](https://www.pytorchlightning.ai/tutorials) to see how it can organize code. This can be handy in later weeks as models get more complex.

## Week 2: Transfer Learning and Fine-Tuning Advanced CNNs

**Goals:** Improve your model’s accuracy and robustness by leveraging state-of-the-art CNN architectures through transfer learning. You will fine-tune a pre-trained model (e.g. ResNet or EfficientNet) on your turtle dataset, learn strategies for efficient fine-tuning, and incorporate better training practices (like learning rate scheduling and regularization). By the end of the week, you should achieve a significant performance boost over the Week 1 baseline and be comfortable using modern CNN backbones.

**Key Topics:**

- **Pre-trained Models & Feature Extractors:** Understand the advantages of using models pre-trained on large datasets (like ImageNet). Models such as **ResNet-50** or **EfficientNet-B0/B4** have learned rich features on generic images that can transfer to turtle photos. You’ll use one as a starting point, replacing or adding a final layer to predict your turtle classes.
- **Fine-Tuning vs. Frozen Features:** Learn the difference between using a pre-trained network as a fixed feature extractor versus fine-tuning its weights. Often, you start with the convolutional layers frozen (to use the learned features as-is) and train only the top classification layer on your data; then gradually unfreeze layers and fine-tune at a low learning rate to adapt features to the turtle domain. This approach takes advantage of general features while not destroying them with high learning rates.
- **Choosing Architectures:** Discuss the trade-offs between models. ResNets are robust and widely used; EfficientNets are known for strong performance with fewer parameters. Consider model size and inference speed if deployment is a concern. Also note frameworks like **timm** (PyTorch Image Models) which provide many architectures and pretrained weights.
- **Training Best Practices:** Use callbacks or utilities to improve training: e.g., **early stopping** (to prevent overfitting), **learning rate scheduling** (lower the LR when fine-tuning deeper layers), and **data augmentation** (continuing from week 1). If using PyTorch Lightning or Keras, utilize their features to simplify these. Ensure you still augment data heavily since the model is more complex now (augmentation helps avoid overfitting).
- **Baseline Comparison:** After fine-tuning, evaluate the model on the validation set. You should see a jump in accuracy or identification performance. This validates the benefit of transfer learning.

**Practical Exercises:**

1. **Setup Transfer Learning:** Load a pretrained model. In PyTorch, you can use `torchvision.models.resnet50(pretrained=True)` and replace the final fully-connected layer with one that has output size = number of turtle IDs. In TensorFlow, you might use `tf.keras.applications.EfficientNetB0` with `include_top=False` to get a feature extractor, then add your own Dense layer for classes.
2. **Initial Training (Feature Extractor):** Freeze the backbone layers (so their weights don’t update) and train only the new output layer on your dataset for a few epochs. This trains the classifier head using the pre-existing features. Monitor performance.
3. **Fine-Tune Deeper Layers:** Unfreeze some of the backbone (e.g., the last couple of layers or blocks) and continue training with a lower learning rate. This will slowly adjust the pre-trained weights to better fit turtle images. Experiment with how many layers to unfreeze and observe effects on validation accuracy.
4. **Experiment with Architectures:** If time permits, try a different model (for instance, compare ResNet50 vs EfficientNet-B3). Use the same fine-tuning procedure. Which model performs better on your data? Note the training time and resource usage differences as well.
5. **Evaluate and Save Model:** Evaluate the fine-tuned model on the validation/test set. Besides accuracy, look at confusion matrix – are certain individuals being confused? Save the model and document the results as the new baseline for identification performance.

**Resources:**

- **PyTorch Transfer Learning Tutorial** – The official PyTorch tutorial on transfer learning provides a step-by-step example of fine-tuning a ResNet on a small dataset ([Transfer Learning for Computer Vision Tutorial - PyTorch](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#:~:text=In%20this%20tutorial%2C%20you%20will,image%20classification%20using%20transfer%20learning)). This can be directly applied to your problem.
- **Keras Transfer Learning Guide** – TensorFlow’s documentation (e.g., the [Fine-tuning an image classification model](https://www.tensorflow.org/tutorials/images/transfer_learning) tutorial) shows how to use a Keras Application model (like EfficientNet) and fine-tune it on new data.
- **PyTorch Lightning Fine-Tuning** – If using Lightning, see the docs on the [Fine-Tuning Callback] for a convenient way to unfreeze layers over epochs. Lightning also provides built-in methods for early stopping and checkpoints.
- **EfficientNet vs ResNet** – Blog posts such as *“PyTorch ResNet: The Basics and a Quick Tutorial”* ([PyTorch ResNet: The Basics and a Quick Tutorial](https://www.run.ai/guides/deep-learning-for-computer-vision/pytorch-resnet#:~:text=PyTorch%20ResNet%3A%20The%20Basics%20and,on%20PyTorch%2C%20with%20code%20examples)) and others on EfficientNet can give context on these architectures. The [timm library README](https://github.com/rwightman/pytorch-image-models) is useful for discovering many pretrained models.
- **Training on Cloud (optional)** – If your dataset or model is too large for a local machine, consider using Google Colab (free GPU runtime) or cloud services. For example, AWS EC2 P2 instances or GCP AI Platform can provide GPUs for training. Google Colab’s free tier can handle moderate training; just remember to save your model to Google Drive or download it after training.

## Week 3: Metric Learning and Face-Recognition-Style Re-Identification

**Goals:** Evolve the system from a closed-set classifier to an open-set recognizer that can tell if two images are the same individual. This week you will explore **metric learning** techniques such as Siamese networks, triplet loss, and specialized losses like ArcFace. The goal is to learn how to obtain **embedding vectors** for images that cluster same-individual images together and apart from others, enabling identification of new individuals without retraining the whole network. By the end of the week, you will implement a prototype face-recognition-style model for turtle re-ID and evaluate it on the task of matching individuals across images.

**Key Topics:**

- **Classification vs. Metric Learning:** Understand the limitation of a pure classification approach for re-identification. With a growing number of individuals, continually adding classes is impractical. Instead, metric learning learns a feature space where images of the same turtle are close and different turtles are far apart. This enables comparing images by distance in that space, not by fixed classes. In fact, past wildlife ID competitions found that directly training for similarity (e.g. Siamese networks) worked better than treating it as a large classification problem ([GitHub - ahmedbesbes/whales-classification: My solution to the Global Data Science Challenge](https://github.com/ahmedbesbes/whales-classification#:~:text=Since%20we%27re%20asked%20to%20list,representation%20vector%20for%20similarity%20measures)).
- **Siamese Network Architecture:** Study the Siamese network design: two identical CNN sub-networks (sharing weights) take two images and output their feature vectors, which are then compared (via a distance or a similarity score). The network is trained with pairs of images labeled “same” or “different” individual. A variant is the **triplet network**, which considers an Anchor, a Positive (same ID as anchor), and a Negative (different ID) in each training step. The loss (e.g. **Triplet Loss**) encourages the anchor to be closer to the positive than to the negative by some margin.
- **Loss Functions for Re-ID:** Explore common loss functions:
  - **Contrastive Loss:** Trains on pairs, pulling same-ID pairs together in embedding space and pushing different-ID pairs apart.
  - **Triplet Loss:** Operates on triplets (anchor, positive, negative) to enforce a margin between positive and negative distances.
  - **Margin Loss (ArcFace/CosFace):** These are modified softmax losses used in face recognition. **ArcFace**, for example, adds an angular margin to the softmax criterion to maximize class separability ([GitHub - ahmedbesbes/whales-classification: My solution to the Global Data Science Challenge](https://github.com/ahmedbesbes/whales-classification#:~:text=The%20arcface%20loss%20has%20been,most%20common%20face%20identification%20benchmarks)). It was originally designed for face ID but can be repurposed for wildlife identification ([SeaTurtleID2022: A Long-Span Dataset for Reliable Sea Turtle Re-Identification](https://openaccess.thecvf.com/content/WACV2024/papers/Adam_SeaTurtleID2022_A_Long-Span_Dataset_for_Reliable_Sea_Turtle_Re-Identification_WACV_2024_paper.pdf#:~:text=ArcFace%20loss%20,angu%02lar%20margin%20m%20to%20improve)). ArcFace often outperforms basic triplet loss on face benchmarks ([GitHub - ahmedbesbes/whales-classification: My solution to the Global Data Science Challenge](https://github.com/ahmedbesbes/whales-classification#:~:text=The%20arcface%20loss%20has%20been,most%20common%20face%20identification%20benchmarks)) by producing highly discriminative features.
- **Implementing Metric Learning:** Consider how to form training batches for metric learning. You will need to sample images of individuals to form pairs or triplets. Hard negative mining (choosing challenging negative examples) can improve training but adds complexity. Libraries like **PyTorch Metric Learning** can automate some of this. Alternatively, using an ArcFace-like approach means still training a classifier (with special loss) but the output features (before the final layer) serve as embeddings.
- **Inference with Embeddings:** Learn how to use the trained embedding model. For identification, you can store one or more feature vectors per known individual (a "gallery"). To identify a new image, compute its embedding and find the closest gallery embedding (e.g. by cosine similarity or Euclidean distance). If the closest distance is above a threshold, you can label it as a new unknown individual (this introduces the open-set aspect, which we’ll refine in Week 4).
- **Comparing Approaches:** There are multiple ways to achieve re-ID; this week’s aim is to implement at least one (e.g. a triplet-loss Siamese network) and understand others conceptually. You might compare the embedding output from your metric-learning model with the embeddings from last week’s softmax model to see which separates individuals better.

**Practical Exercises:**

1. **Siamese Network Implementation:** Build a Siamese network using a common CNN backbone (you can use the fine-tuned model from Week 2 as a starting point for the sub-network). In PyTorch, you might create a model class that takes two images, passes each through the CNN (shared weights) to get two feature vectors, then outputs the distance between them. In Keras, you can use the functional API to create two branches that merge with a distance layer.
2. **Triplet Loss Training:** Instead of explicit pair input, consider training using triplet loss. You’ll need to generate triplets: pick an anchor image, a positive of the same turtle, and a negative of a different turtle. One simple strategy is for each anchor in a batch, randomly choose one other image of the same turtle as positive, and a random image of a different turtle as negative. More advanced is hard-negative mining (finding a negative that is somewhat similar to the anchor to make learning harder). Use a framework or write a custom training loop to compute triplet loss (`torch.nn.TripletMarginLoss` is available in PyTorch ([Implementing Content-Based Image Retrieval With Siamese Networks in PyTorch](https://neptune.ai/blog/content-based-image-retrieval-with-siamese-networks#:~:text=Image%3A%20siamese%20network))).
3. **ArcFace (Advanced Option):** If you prefer not to manage triplets, try integrating an ArcFace loss layer. For example, use an open-source implementation (like the one from the *InsightFace* project) or the **PyTorch Metric Learning** library which includes an ArcFaceLoss. Train your network as if it were doing classification, but with the ArcFace loss replacing the standard softmax loss. This will learn an embedding internally. (This exercise is more advanced and can be skipped if focusing on the Siamese approach).
4. **Evaluate Re-ID Performance:** After training, evaluate the model in a retrieval scenario. For each turtle in a test set, take one image as a query and see if the model finds the matching individual among a gallery of others. Compute the **Rank-1 accuracy** (does the top match have the same identity?) and perhaps **Rank-5** (is the true match in the top-5 predictions?). Also compute the **mean Average Precision (mAP)** across queries, which is a common metric in re-identification ([person-reidentification-retail-0277 — OpenVINO™  documentation](https://docs.openvino.ai/2023.3/omz_models_model_person_reidentification_retail_0277.html#:~:text=The%20cumulative%20matching%20curve%20,the%20precision%20and%20recall%20curve)). *Rank-1* measures the chance that the correct ID is the closest match, and *mAP* summarizes how well the model’s ranking of candidates is on average ([person-reidentification-retail-0277 — OpenVINO™  documentation](https://docs.openvino.ai/2023.3/omz_models_model_person_reidentification_retail_0277.html#:~:text=The%20cumulative%20matching%20curve%20,the%20precision%20and%20recall%20curve)).
5. **Visualization:** Take a sample of embeddings and use a tool like TSNE or PCA to project them to 2D. Plot different individuals with different colors. You should see clustering by individual if the metric learning worked. This visual check can be satisfying and reveals if some individuals are still mixed up.
6. **Compare with Baselines:** Finally, compare the new approach to your Week 2 classifier. For example, how does its Rank-1 accuracy compare to the classifier’s accuracy? You can also try using the Week 2 model’s penultimate layer as embeddings and evaluate the retrieval performance to see the difference. Document these findings.

**Resources:**

- **Siamese Network Tutorials:** *“A Friendly Introduction to Siamese Networks”* on BuiltIn or the Keras example *“Image similarity with a Siamese Network”* provide step-by-step guidance. The Keras example (using a triplet loss) is particularly relevant ([Image similarity estimation using a Siamese Network with a triplet loss](https://keras.io/examples/vision/siamese_network/#:~:text=match%20at%20L344%20Our%20Siamese,learn%20to%20separate%20these%20embeddings)) ([Image similarity estimation using a Siamese Network with a triplet loss](https://keras.io/examples/vision/siamese_network/#:~:text=Our%20Siamese%20Network%20will%20generate,learn%20to%20separate%20these%20embeddings)), demonstrating how to use ResNet50 as a backbone and implement triplet loss.
- **PyTorch Metric Learning Library** – This library by Kevin Musgrave streamlines metric learning. It provides ready-made loss functions (Contrastive, Triplet, ArcFace, etc.) and miners for hard negatives. See the [GitHub repo](https://github.com/KevinMusgrave/pytorch-metric-learning) and the documentation for examples of training loops.
- **ArcFace and Face Recognition** – The ArcFace paper (Deng et al. 2019) and various summaries can help understand the concept. A Kaggle post by a whale-ID competitor explains ArcFace in simpler terms ([GitHub - ahmedbesbes/whales-classification: My solution to the Global Data Science Challenge](https://github.com/ahmedbesbes/whales-classification#:~:text=The%20arcface%20loss%20has%20been,most%20common%20face%20identification%20benchmarks)) ([GitHub - ahmedbesbes/whales-classification: My solution to the Global Data Science Challenge](https://github.com/ahmedbesbes/whales-classification#:~:text=When%20given%20a%20feature%20vector,space%20and%20applies%20a%20softmax)). In essence: *“ArcFace maximizes class separability by projecting features to an angular space and adding a margin between classes before applying softmax”* ([GitHub - ahmedbesbes/whales-classification: My solution to the Global Data Science Challenge](https://github.com/ahmedbesbes/whales-classification#:~:text=When%20given%20a%20feature%20vector,space%20and%20applies%20a%20softmax)). This is a powerful idea that you might incorporate. GitHub implementations of ArcFace (e.g., [ronghuaiyang/arcface-pytorch](https://github.com/ronghuaiyang/arcface-pytorch)) could be useful if you choose that route.
- **Example Projects:** Look at similar identification tasks for inspiration. The humpback whale ID challenge solutions (like Ahmed Besbes’s [whale classification repo](https://github.com/ahmedbesbes/whales-classification)) detail how metric learning was used (margin loss, triplet) and how ArcFace boosted performance ([GitHub - ahmedbesbes/whales-classification: My solution to the Global Data Science Challenge](https://github.com/ahmedbesbes/whales-classification#:~:text=Since%20we%27re%20asked%20to%20list,representation%20vector%20for%20similarity%20measures)) ([GitHub - ahmedbesbes/whales-classification: My solution to the Global Data Science Challenge](https://github.com/ahmedbesbes/whales-classification#:~:text=I%20tried%20triplet%20loss%20with,I%20oubviously%20went%20for%20it)). Although whales differ from turtles, the problem structure is analogous.
- **Distance Metrics:** Refresher on cosine vs Euclidean distance. Cosine similarity is often used in face/turtle ID because absolute vector scale might not matter if embeddings are normalized (ArcFace embeddings lie on a hypersphere ([SeaTurtleID2022: A Long-Span Dataset for Reliable Sea Turtle Re-Identification](https://openaccess.thecvf.com/content/WACV2024/papers/Adam_SeaTurtleID2022_A_Long-Span_Dataset_for_Reliable_Sea_Turtle_Re-Identification_WACV_2024_paper.pdf#:~:text=ArcFace%20loss%20,angu%02lar%20margin%20m%20to%20improve))). Be sure to normalize embeddings before comparing if using cosine similarity.

## Week 4: Open-Set Recognition, Tracking New Individuals, and Deployment Considerations

**Goals:** In the final week, focus on transitioning your system to practical usage. This means handling the introduction of new turtles (open-set recognition), setting up processes to **add and track individuals** over time, evaluating the system with appropriate metrics, and exploring options to deploy or scale the solution (either on local hardware or the cloud). By the end of this week, you should have a plan for maintaining the system long-term and an understanding of how to deploy the model for real-world use.

**Key Topics:**

- **Open-Set Recognition:** In real field usage, the model will encounter turtles it hasn’t seen before. The system should ideally recognize when a new image does not match any known individual and flag it as a new turtle. Discuss methods to achieve this:
  - **Distance Thresholding:** One simple approach is to set a similarity threshold in the embedding space. If the closest match’s distance is above a certain threshold, treat the query as an unknown individual ([Open-Set Events Identification Based on Deep Metric-Learning for ...](https://ieeexplore.ieee.org/document/9130778/#:~:text=By%20setting%20the%20appropriate%20threshold%2C,set)). Choosing this threshold might involve tuning on a validation set (balancing false alarms vs missed new IDs).
  - **Open-World Learning:** Note that open-set recognition is an active research area. Sophisticated methods (like OpenMax or classifier threshold learning ([Open-Set Events Identification Based on Deep Metric-Learning for ...](https://ieeexplore.ieee.org/document/9130778/#:~:text=Open,set))) exist, but a practical approach for now is to use the embedding model plus a threshold or an outlier detector. When an “unknown” is detected, you can assign a new ID and add its embeddings to the gallery for future recognition.
  - **Incremental Training:** Consider how you will update the model when new individuals are added. Ideally, a good embedding model will generalize to new IDs without retraining. But over time, you might retrain or fine-tune the model with the expanded dataset to improve overall performance. Techniques like **continual learning** or fine-tuning with a small learning rate on new data can be used while hopefully not forgetting old identities.
- **Individual Tracking & Database:** As the number of turtle IDs grows, maintain a database of known individuals. This can simply be a directory of images per ID or a more advanced vector database of embeddings (e.g., using FAISS or an approximate nearest neighbor index for scalability). Plan how to store metadata for each individual (name/ID, example images, date first seen, etc.) if needed for a field application.
- **Evaluation Metrics for Re-ID:** Ensure you are evaluating the system properly for the intended use. For closed-set classification, accuracy and confusion matrices were fine. But for open-set re-identification, use metrics like:
  - **CMC (Cumulative Match Characteristic) curve and Rank-k accuracy:** as introduced in Week 3, Rank-1 accuracy is a primary metric (the probability that the top match is correct) ([person-reidentification-retail-0277 — OpenVINO™  documentation](https://docs.openvino.ai/2023.3/omz_models_model_person_reidentification_retail_0277.html#:~:text=The%20cumulative%20matching%20curve%20,the%20precision%20and%20recall%20curve)). You can plot a CMC curve to see the identification rate at rank-1, rank-5, rank-10, etc.
  - **Mean Average Precision (mAP):** This is popular in person-reID literature. It averages the precision of the identification across all queries ([person-reidentification-retail-0277 — OpenVINO™  documentation](https://docs.openvino.ai/2023.3/omz_models_model_person_reidentification_retail_0277.html#:~:text=The%20cumulative%20matching%20curve%20,the%20precision%20and%20recall%20curve)). A higher mAP means that on average the model returns relevant images at the top of the ranking for each query.
  - **Verification metrics:** If treating it like face verification (same/different decisions), you can compute true positive/false positive rates at various thresholds, and an ROC curve or recall-precision curve for the “is this the same turtle?” decision.
  - Use a dedicated evaluation script to measure these on a test set where some individuals might be held-out as “new” to test open-set performance (i.e., make the model try to label them unknown).
- **Tools & Libraries for Deployment:** Now that a model is trained, consider how to deploy it:
  - **Local Deployment:** If you have a local GPU or even CPU, you might build a small application. This could be a command-line tool or a web app (using Flask/FastAPI) that allows a user to upload a turtle photo and get the identity. Frameworks like **ONNX** can be used to export the model for efficient inference on different platforms.
  - **Cloud Deployment:** If you need to serve a larger community (e.g., researchers uploading photos), you could deploy the model on a cloud service. Options include AWS SageMaker endpoints (managed model deployment), a REST API on an EC2 or GCP VM with the model, or using serverless functions (though for deep learning, a persistent server is usually needed for loading the model into memory). Ensure to consider cost and scalability (GPU instances are expensive; you might use a CPU inference if real-time speed is not critical or use GPU only when needed).
  - **Pipeline for Ongoing Training:** If new data comes in frequently, you might set up a schedule or pipeline for re-training or fine-tuning the model every so often (monthly, etc.) using the new data, and then redeploy the updated model.
  - **Experiment Tracking:** As you’ve done multiple training runs, tools like **Weights & Biases** or **TensorBoard** (or Lightning’s built-in logger) can log experiments. This is more for your development process, but it’s good practice to track hyperparameters and results, especially as the project grows.
- **Real-World Considerations:** Discuss any additional steps needed for a production-quality system:
  - Incorporating a detection step if images might have multiple turtles or other objects (e.g., first run an object detector to crop the turtle’s face or shell pattern region before recognition).
  - Handling poor quality images: maybe a pre-processing step to enhance underwater images (there are deep learning models for underwater image enhancement, which could be a future improvement).
  - User feedback loop: in a conservation context, a human might verify the model’s suggestion and correct it, which means you should be able to update the database (e.g., merge two IDs if the model thought they were separate, or split if a mistake was made).
  
**Practical Exercises:**

1. **Unknown Detection Test:** Take a few images of a particular turtle that the model was **not** trained on (or simulate by holding out one individual from training). Run these through your identification pipeline (embedding + nearest neighbor search among known IDs). Implement a simple threshold on distance: if the nearest neighbor distance is greater than some value, output “Unknown individual”. Experiment to find a reasonable threshold by checking a mix of known vs unknown queries. (For example, you might compute the distribution of distances for matching pairs vs non-matching pairs from your validation set to decide a threshold that yields a low false positive rate).
2. **Add New Individual:** Simulate the addition of a new turtle to the system. Take that “unknown” turtle’s images, assign a new ID in the database, and add its embeddings to your gallery. Now verify the system can recognize this new ID in subsequent queries. This exercise ensures your system’s architecture supports incremental growth without retraining every time.
3. **Compute Final Metrics:** Using a test dataset, preferably with the realistic scenario (some individuals missing from training to test open-set behavior), compute Rank-1, Rank-5 accuracy and mAP. You can use existing libraries or write a small script. For each query image of a turtle, rank all gallery images by cosine similarity and check if the correct match is in the top K. Calculate precision/recall for each query to get AP and then average for mAP. **Note:** If this is too involved, at least compute the Rank-1 accuracy and perhaps the false alarm rate for unknown detection. These will be key numbers to report.
4. **Deployment Exercise:** Choose a deployment path and do a small prototype. For instance, try exporting your model (if PyTorch, maybe script it with TorchScript or export to ONNX; if TensorFlow, save it as a SavedModel). Then write a minimal inference script that loads the model and processes a new image to output an identity. This could be the backbone of a deployed service. If feasible, set up a simple local server (Flask app) that exposes an endpoint for identification. If not, at least document how you would deploy on, say, AWS: e.g., using a Docker container with your model and a web service.
5. **Future Directions:** Write down a brief plan for future improvements: for example, “incorporate automated head detection to crop input images to just the turtle’s head pattern”, or “collect more data for individuals where the model is confusing”, or “explore a dedicated re-ID library like Torchreid for more advanced models once concept is proven”. This solidifies your understanding of how the project can evolve beyond the 4-week curriculum.

**Resources:**

- **Open-Set Recognition Literature:** For a deeper dive, see *“Recent Advances in Open Set Recognition: A Survey”* ([[PDF] Recent Advances in Open Set Recognition: A Survey - arXiv](https://arxiv.org/pdf/1811.08581#:~:text=,classifiers%20to%20not%20only)). It’s beyond our scope to implement those methods, but it provides context. A simpler read is this Reddit discussion on handling an “other” class in classification which mentions open-set recognition approaches ([[D] "Other" class in DNN classification : r/MachineLearning - Reddit](https://www.reddit.com/r/MachineLearning/comments/bdktjv/d_other_class_in_dnn_classification/#:~:text=%5BD%5D%20,networks%20need%20to%20be)).
- **FAISS for Similarity Search:** If your gallery of known individuals grows large, look into Facebook’s [FAISS library](https://faiss.ai/) for efficient nearest neighbor search on embeddings. It's an advanced tool that can handle millions of vectors quickly, useful if scaling up the photo database.
- **Evaluation Tools:** The pyMetricLearning library (mentioned earlier) also has utilities for evaluation. Alternatively, consider using open-source re-ID evaluation scripts. For example, the Torchreid library’s evaluation code or OpenVINO’s documentation on re-ID metrics (which we cited) gives formulas for Rank-1 and mAP ([person-reidentification-retail-0277 — OpenVINO™  documentation](https://docs.openvino.ai/2023.3/omz_models_model_person_reidentification_retail_0277.html#:~:text=The%20cumulative%20matching%20curve%20,the%20precision%20and%20recall%20curve)). These can guide your own implementation.
- **Cloud Deployment Guides:** If deploying to AWS, check Amazon’s examples for deploying PyTorch models on Sagemaker. Google Cloud’s [Vertex AI](https://cloud.google.com/vertex-ai) has a section on custom model deployment as well. For something quick, you can also use **Gradio** or **Streamlit** to build a simple web UI for the model and share it (for instance, a Gradio app can be launched on Hugging Face Spaces for free).
- **Continual Learning** (optional reading): If you’re interested in techniques to update the model without full retraining, look up “class-incremental learning” or frameworks like *Adaptive Deep Learning*. This is an advanced topic, but relevant if the system is in long-term use with many new classes.

---

By following this curriculum, you will progressively build a sophisticated system for sea turtle identification. You started with understanding the unique visual features of turtles and basic CNN classification, moved on to harnessing pre-trained models for better accuracy, then transitioned to metric learning for open-set identification, and finally addressed how to maintain and deploy the system in a real-world scenario. Throughout, you applied both theory and practice, from data augmentation to advanced loss functions and cloud considerations. This hands-on journey should equip you with not only a functioning prototype for turtle re-identification, but also a deeper insight into person/animal re-identification challenges and solutions in deep learning ([SeaTurtleID2022: A Long-Span Dataset for Reliable Sea Turtle Re-Identification](https://openaccess.thecvf.com/content/WACV2024/papers/Adam_SeaTurtleID2022_A_Long-Span_Dataset_for_Reliable_Sea_Turtle_Re-Identification_WACV_2024_paper.pdf#:~:text=ArcFace%20loss%20,angu%02lar%20margin%20m%20to%20improve)) ([person-reidentification-retail-0277 — OpenVINO™  documentation](https://docs.openvino.ai/2023.3/omz_models_model_person_reidentification_retail_0277.html#:~:text=The%20cumulative%20matching%20curve%20,the%20precision%20and%20recall%20curve)). Good luck with your project, and enjoy the process of learning and discovery! 

